{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f3d19-ae28-4810-8bd7-e257b8b173ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your DataFrame (assuming it's already loaded as 'df')\n",
    "df = pd.read_csv(\"England CSV.csv\")\n",
    "df.head()\n",
    "# Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "# Check for impossible values (e.g., negative goals)\n",
    "print(df[['FTH Goals', 'FTA Goals']].describe())\n",
    "# Clip extreme shots/corners (if needed)\n",
    "df['HS'] = df['H Shots'].clip(upper=df['H Shots'].quantile(0.99))\n",
    "# Feature engineering (better than outlier removal)\n",
    "df['Home_GoalDiff'] = df['FTH Goals'] - df['FTA Goals']\n",
    "# Keep key stats and engineer ratios:\n",
    "df['Shot_Ratio'] = df['H Shots'] / (df['A Shots'] + 1e-6)  # Avoid division by zero\n",
    "df['Corner_Ratio'] = df['H Corners'] / (df['A Corners'] + 1e-6)\n",
    "columns_to_drop = ['Season', 'Referee', 'H SOT', 'A SOT', 'H Shots', 'A Shots', 'H Fouls', 'A Fouls', 'H Corners', 'A Corners', 'A Yellow', 'H Yellow', 'H Red', 'A Red', 'League', 'Display_Order']\n",
    "df_reduced = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# 1. Encode team names\n",
    "le_teams = LabelEncoder()\n",
    "df_reduced['HomeTeam'] = le_teams.fit_transform(df_reduced['HomeTeam'])\n",
    "df_reduced['AwayTeam'] = le_teams.transform(df_reduced['AwayTeam'])\n",
    "\n",
    "# 2. Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "df_reduced[['HS', 'Home_GoalDiff']] = scaler.fit_transform(df_reduced[['HS', 'Home_GoalDiff']])\n",
    "\n",
    "# 3. Extract time features\n",
    "df_reduced['Date'] = pd.to_datetime(df_reduced['Date'])\n",
    "df_reduced['Year'] = df_reduced['Date'].dt.year\n",
    "df_reduced['Month'] = df_reduced['Date'].dt.month\n",
    "df_reduced.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder_result = LabelEncoder()\n",
    "df_reduced['FT Result Encoded'] = label_encoder_result.fit_transform(df_reduced['FT Result'])\n",
    "df_reduced.drop('FT Result', axis=1, inplace=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sort by date\n",
    "df_reduced = df_reduced.sort_values('Year') # Sorting by year as 'Date' is already removed\n",
    "\n",
    "# Split into features (X) and target (y)\n",
    "X = df_reduced.drop('FT Result Encoded', axis=1)  # Features\n",
    "y = df_reduced['FT Result Encoded']             # Target (0, 1, 2)\n",
    "\n",
    "# Time-based split\n",
    "train_size = int(0.8 * len(df_reduced))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# 2. Data Preparation (Scaling - moved here to be consistent)\n",
    "scaler_final = StandardScaler()\n",
    "X_train_scaled = scaler_final.fit_transform(X_train)\n",
    "X_test_scaled = scaler_final.transform(X_test)\n",
    "\n",
    "# Convert scaled arrays back to DataFrames (optional, but good practice for clarity)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# 3. Model Initialization\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'),\n",
    "    \"Logistic Regression\": LogisticRegression(multi_class='multinomial',\n",
    "                                               solver='lbfgs',\n",
    "                                               max_iter=1000)\n",
    "}\n",
    "\n",
    "# 4. Model Training and Evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        if name == \"Logistic Regression\":\n",
    "            model.fit(X_train_scaled, y_train) # Use scaled data and original y_train (which is now encoded)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train) # Use scaled data and original y_train (which is now encoded)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        results[name] = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'report': classification_report(y_test, y_pred,\n",
    "                                            target_names=label_encoder_result.classes_) # Use the correct encoder's classes\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Print the results\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(\"Classification Report:\\n\", result['report'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
